{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Document Q&A with pgvector\n",
        "PDF → chunk → embed → store in Postgres/pgvector → retrieve top-k similar chunks.\n",
        "\n",
        "## Prereqs\n",
        "From repo root:\n",
        "```bash\n",
        "docker compose up -d\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'postgresql+psycopg://ai:ai@localhost:5532/ai'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "PG_USER = os.getenv(\"POSTGRES_USER\", \"ai\")\n",
        "PG_PASS = os.getenv(\"POSTGRES_PASSWORD\", \"ai\")\n",
        "PG_DB   = os.getenv(\"POSTGRES_DB\", \"ai\")\n",
        "PG_HOST = os.getenv(\"POSTGRES_HOST\", \"localhost\")\n",
        "PG_PORT = os.getenv(\"POSTGRES_PORT\", \"5532\")\n",
        "\n",
        "DB_URL = f\"postgresql+psycopg://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{PG_DB}\"\n",
        "DB_URL\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verify DB connectivity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connection attempt 1/12\n",
            "✅ Connected: PostgreSQL 15.4 (Debian 15.4-2.pgdg120+1) on x86_64-pc-linux-gnu\n",
            "✅ pgvector extension ready\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from sqlalchemy import create_engine, text\n",
        "from sqlalchemy.exc import OperationalError\n",
        "\n",
        "def test_db_connection(db_url: str, max_retries: int = 12, wait_time: int = 2) -> None:\n",
        "    engine = create_engine(db_url, pool_pre_ping=True)\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            print(f\"Connection attempt {attempt}/{max_retries}\")\n",
        "            with engine.connect() as conn:\n",
        "                version = conn.execute(text(\"SELECT version();\")).scalar_one()\n",
        "                print(\"✅ Connected:\", version.split(\",\")[0])\n",
        "                conn.execute(text(\"CREATE EXTENSION IF NOT EXISTS vector;\"))\n",
        "                print(\"✅ pgvector extension ready\")\n",
        "            return\n",
        "        except OperationalError as e:\n",
        "            print(\"⏳ Not ready yet:\", e.__class__.__name__)\n",
        "            time.sleep(wait_time)\n",
        "    raise RuntimeError(\"❌ Database connection failed. Is docker compose up?\")\n",
        "\n",
        "test_db_connection(DB_URL)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DocumentQA implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import io\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, List\n",
        "\n",
        "import numpy as np\n",
        "import requests\n",
        "from pypdf import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sqlalchemy import create_engine, text\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class RetrievalHit:\n",
        "    chunk_index: int\n",
        "    content: str\n",
        "\n",
        "\n",
        "class DocumentQA:\n",
        "    def __init__(\n",
        "        self,\n",
        "        db_url: str,\n",
        "        model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        table_name: str = \"doc_chunks\",\n",
        "    ):\n",
        "        self.db_url = db_url\n",
        "        self.table = table_name\n",
        "\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.dim = self.model.get_sentence_embedding_dimension()\n",
        "\n",
        "        self.engine = create_engine(self.db_url, pool_pre_ping=True)\n",
        "        self._init_db()\n",
        "\n",
        "        self.current_source_url: Optional[str] = None\n",
        "\n",
        "    def _init_db(self) -> None:\n",
        "        with self.engine.begin() as conn:\n",
        "            conn.execute(text(\"CREATE EXTENSION IF NOT EXISTS vector;\"))\n",
        "            conn.execute(text(f\"\"\"\n",
        "                CREATE TABLE IF NOT EXISTS {self.table} (\n",
        "                    id BIGSERIAL PRIMARY KEY,\n",
        "                    source_url TEXT NOT NULL,\n",
        "                    chunk_index INT NOT NULL,\n",
        "                    content TEXT NOT NULL,\n",
        "                    embedding vector({self.dim}) NOT NULL\n",
        "                );\n",
        "            \"\"\"))\n",
        "\n",
        "            conn.execute(text(\n",
        "                f\"CREATE INDEX IF NOT EXISTS {self.table}_embedding_ivfflat \"\n",
        "                f\"ON {self.table} USING ivfflat (embedding vector_l2_ops) \"\n",
        "                f\"WITH (lists = 100);\"\n",
        "            ))\n",
        "\n",
        "    def _download_pdf(self, url: str) -> bytes:\n",
        "        r = requests.get(url, timeout=60)\n",
        "        r.raise_for_status()\n",
        "        return r.content\n",
        "\n",
        "    def _extract_text(self, pdf_bytes: bytes) -> str:\n",
        "        reader = PdfReader(io.BytesIO(pdf_bytes))\n",
        "        pages = [(p.extract_text() or \"\") for p in reader.pages]\n",
        "        text_all = \"\\n\".join(pages).strip()\n",
        "        if not text_all:\n",
        "            raise ValueError(\"No text extracted. PDF may be scanned/image-only.\")\n",
        "        return text_all\n",
        "\n",
        "    def _chunk_text(self, text_str: str, chunk_chars: int = 1200, overlap: int = 150) -> List[str]:\n",
        "        text_str = \" \".join(text_str.split())\n",
        "        chunks: List[str] = []\n",
        "        start = 0\n",
        "        while start < len(text_str):\n",
        "            end = min(len(text_str), start + chunk_chars)\n",
        "            chunks.append(text_str[start:end])\n",
        "            if end == len(text_str):\n",
        "                break\n",
        "            start = max(0, end - overlap)\n",
        "        return chunks\n",
        "\n",
        "    def load_pdf_url(\n",
        "        self,\n",
        "        url: str,\n",
        "        chunk_chars: int = 1200,\n",
        "        overlap: int = 150,\n",
        "        delete_existing_for_url: bool = True,\n",
        "    ) -> int:\n",
        "        pdf_bytes = self._download_pdf(url)\n",
        "        text_all = self._extract_text(pdf_bytes)\n",
        "        chunks = self._chunk_text(text_all, chunk_chars=chunk_chars, overlap=overlap)\n",
        "\n",
        "        embeddings = self.model.encode(chunks, convert_to_numpy=True, show_progress_bar=True).astype(np.float32)\n",
        "\n",
        "        with self.engine.begin() as conn:\n",
        "            if delete_existing_for_url:\n",
        "                conn.execute(text(f\"DELETE FROM {self.table} WHERE source_url = :u\"), {\"u\": url})\n",
        "\n",
        "            for i, (chunk, emb) in enumerate(zip(chunks, embeddings)):\n",
        "                conn.execute(\n",
        "                    text(f\"\"\"\n",
        "                        INSERT INTO {self.table} (source_url, chunk_index, content, embedding)\n",
        "                        VALUES (:url, :idx, :content, :embedding)\n",
        "                    \"\"\"),\n",
        "                    {\"url\": url, \"idx\": i, \"content\": chunk, \"embedding\": emb.tolist()},\n",
        "                )\n",
        "\n",
        "            conn.execute(text(f\"ANALYZE {self.table};\"))\n",
        "\n",
        "        self.current_source_url = url\n",
        "        print(f\"✅ Loaded {len(chunks)} chunks into pgvector.\")\n",
        "        return len(chunks)\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int = 5, source_url: Optional[str] = None) -> List[RetrievalHit]:\n",
        "        source_url = source_url or self.current_source_url\n",
        "        if not source_url:\n",
        "            raise ValueError(\"No source_url set. Call load_pdf_url(...) first or pass source_url=...\")\n",
        "\n",
        "        q_emb = self.model.encode(query, convert_to_numpy=True).astype(np.float32)\n",
        "\n",
        "        with self.engine.begin() as conn:\n",
        "            rows = conn.execute(\n",
        "                text(f\"\"\"\n",
        "                    SELECT chunk_index, content\n",
        "                    FROM {self.table}\n",
        "                    WHERE source_url = :url\n",
        "                    ORDER BY embedding <-> (:qvec)::vector\n",
        "                    LIMIT :k\n",
        "                \"\"\"),\n",
        "                {\"url\": source_url, \"qvec\": q_emb.tolist(), \"k\": top_k},\n",
        "            ).fetchall()\n",
        "\n",
        "        return [RetrievalHit(chunk_index=r[0], content=r[1]) for r in rows]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load a PDF and retrieve chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20d59fbeb83044a3ba2e0ad7a3e7ec6c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded 238 chunks into pgvector.\n",
            "\n",
            "--- chunk 217\n",
            "es” document published under https://www.nestle.com/investors/publications provides the definition of these non-IFRS Accounting Standards financial performance measures. (b) Calculated on the basis of the dividend for the year concerned, which is paid in the following year, and on high/low stock prices. (c) As proposed by the Board of Directors of Nestlé S.A. (d) 2024 and 2023 based on headcount. Other years based on full-time equivalent. Financial information – 5 year review Consolidated Financial Statements of the Nestlé Group 2024190 158th Financial Statements of Nestlé S.A. 158th Financial ...\n",
            "\n",
            "--- chunk 191\n",
            "tlé Group 2024176 City of operations Consolidated Financial Statements of the Nestlé Group 2024 177 To the General Meeting of Lausanne, 12 February 2025 Nestlé S.A., Cham & Vevey Report of the statutory auditor Report on the audit of the consolidated financial statements Opinion We have audited the consolidated financial statements of Nestlé S.A. and its subsidiaries (the Group), which comprise the consolidated balance sheet as at 31 December 2024, the consolidated income statement, the consolidated statement of comprehensive income, the consolidated cash flow statement and the consolidated st ...\n",
            "\n",
            "--- chunk 0\n",
            "Financial Statements 2024 Consolidated Financial Statements of the Nestlé Group 2024 158th Financial Statements of Nestlé S.A. Consolidated Financial Statements of the Nestlé Group 2024 77 Consolidated Financial Statements of the Nestlé Group 2024 Consolidated Financial Statements of the Nestlé Group 202478 79 80 81 82 84 85 86 86 88 91 100 102 103 106 110 116 126 129 144 148 150 151 153 158 159 160 Principal exchange rates Consolidated income statement for the year ended December 31, 2024 Consolidated statement of comprehensive income for the year ended December 31, 2024 Consolidated balance  ...\n",
            "\n",
            "--- chunk 233\n",
            "eive the dividend is April 17, 2025. The shares will be traded ex-dividend as of April 22, 2025. The net dividend will be payable as from April 24, 2025. The Board of Directors Cham and Vevey, February 12, 2025 158th Financial Statements of Nestlé S.A. 203 158th Financial Statements of Nestlé S.A. 204 To the General Meeting of Lausanne, 12 February 2025 Nestlé S.A., Cham & Vevey Report of the statutory auditor Report on the audit of the financial statements Opinion We have audited the financial statements of Nestlé S.A. (the Company), which comprise the balance sheet as at 31 December 2024, th ...\n",
            "\n",
            "--- chunk 231\n",
            "will be delivered at the end of the Board year in April 2025. They will be valued at the closing price of the share on the SIX Swiss Exchange on the ex-dividend date 2025. The actual number of shares delivered will be published in the Nestlé S.A. 2025 Financial Statements. In 2024, 68 737 shares were delivered to the Board. 19. Full-time equivalents For Nestlé S.A., the annual average number of full-time equivalents for the reporting year, as well as the previous year, did not exceed 250. 20. Events after the balance sheet date There are no subsequent events which either warrant a modification ...\n"
          ]
        }
      ],
      "source": [
        "qa = DocumentQA(DB_URL)\n",
        "\n",
        "pdf_url = \"https://www.nestle.com/sites/default/files/2025-02/2024-financial-statements-en.pdf\"\n",
        "qa.load_pdf_url(pdf_url)\n",
        "\n",
        "hits = qa.retrieve(\"What are the main insights from Nestle's 2024 financial statements?\", top_k=5)\n",
        "for h in hits:\n",
        "    print(\"\\n--- chunk\", h.chunk_index)\n",
        "    print(h.content[:600], \"...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d575f12",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
